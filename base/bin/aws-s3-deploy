#!/usr/bin/env bash
set -ex

DEFAULT_CACHE_CONTROL='--cache-control "max-age=3600"'

# Src: https://stackoverflow.com/questions/192249/how-do-i-parse-command-line-arguments-in-bash
POSITIONAL=()
while [[ $# -gt 0 ]]; do
  case ${1} in
    --dryrun)
    DRYRUN="${1}"
    shift # past argument
    ;;
    --cache-control)
    CACHE_CONTROL="${1} \"${2}\""
    shift # past argument
    shift # past value
    ;;
    --distribution-id)
    AWS_CLOUDFRONT_DISTRIBUTION_ID="${2}"
    shift # past argument
    shift # past value
    ;;
    --region)
    AWS_REGION="${2}"
    shift # past argument
    shift # past value
    ;;
    --profile)
    AWS_PROFILE="${2}"
    shift # past argument
    shift # past value
    ;;
    *)    # unknown option
    POSITIONAL+=("$1") # save it in an array for later
    shift # past argument
    ;;
  esac
done
set -- "${POSITIONAL[@]}" # restore positional parameters

BUCKET_NAME=$1
SOURCE_DIR=${2:-./}
BUCKET_PATH=${3:-/}

AWS_REGION=${AWS_REGION:-${AWS_DEFAULT_REGION}}
AWS_PROFILE=${AWS_PROFILE:-${AWS_DEFAULT_PROFILE}}

CACHE_CONTROL=${CACHE_CONTROL:-${DEFAULT_CACHE_CONTROL}}

# Ensure required vars a set
if [ -z "${BUCKET_NAME}" ]; then
  echo "Destination bucket name required. 'aws-s3-deploy \${source_dir} \${bucket_name}'";
  exit 1;
fi


# ** Modify below to fit your specific needs **

# Clean up
#aws s3 rm s3://${BUCKET_NAME} --recursive --include "*"

# Docs http://docs.aws.amazon.com/cli/latest/reference/s3/sync.html
# --dryrun (boolean) Displays the operations that would be performed using the specified command without actually running them.
# --quiet (boolean) Does not display the operations performed from the specified command.
# --include (string) Don't exclude files or objects in the command that match the specified pattern. See Use of Exclude and Include Filters for details.
# --exclude (string) Exclude all files or objects from the command that matches the specified pattern.
# --sse (string) Specifies server-side encryption of the object in S3. Valid values are AES256 and aws:kms. If the parameter is specified but no value is provided, AES256 is used.
# --content-type (string) Specify an explicit content type for this operation. This value overrides any guessed mime types.
# --cache-control (string) Specifies caching behavior along the request/reply chain.
# --content-language (string) The language the content is in.
# --expires (string) The date and time at which the object is no longer cacheable.
# --metadata (map) A map of metadata to store with the objects in S3. This will be applied to every object which is part of this request. In a sync, this means that files which haven't changed won't receive the new metadata. When copying between two s3 locations, the metadata-directive argument will default to 'REPLACE' unless otherwise specified. http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-metadata
# --metadata-directive (string) Specifies whether the metadata is copied from the source object or replaced with metadata provided when copying S3 objects. Note that if the object is copied over in parts, the source object's metadata will not be copied over, no matter the value for --metadata-directive, and instead the desired metadata values must be specified as parameters on the command line. Valid values are COPY and REPLACE. If this parameter is not specified, COPY will be used by default. If REPLACE is used, the copied object will only have the metadata values that were specified by the CLI command. Note that if you are using any of the following parameters: --content-type, content-language, --content-encoding, --content-disposition, --cache-control, or --expires, you will need to specify --metadata-directive REPLACE for non-multipart copies if you want the copied objects to have the specified metadata values.
# --delete (boolean) Files that exist in the destination but not in the source are deleted during sync.

echo "s3 sync ${SOURCE_DIR} => s3://${BUCKET_NAME}${BUCKET_PATH}"

# TODO add in `--content-encoding "gzip"` `--content-encoding "br"`
# TODO pre-zip files?
copy() {
    echo "copy ${1} ${2}"
    aws s3 cp ${SOURCE_DIR} s3://${BUCKET_NAME}${BUCKET_PATH} --recursive --sse \
        --no-guess-mime-type --content-type "${2}" --metadata-directive "REPLACE" \
        --exclude="*" --include "${1}" \
        ${CACHE_CONTROL} ${DRYRUN} \
        --profile ${AWS_PROFILE}
}

sync() {
    echo "sync ${1} ${2}"
    aws s3 sync ${SOURCE_DIR} s3://${BUCKET_NAME}${BUCKET_PATH} --sse \
        --no-guess-mime-type --content-type "${2}" --metadata-directive "REPLACE" \
        --exclude="*" --include "${1}" \
        ${CACHE_CONTROL} ${DRYRUN} \
        --profile ${AWS_PROFILE}
}

# /etc/mime.types
sync "*.txt"         "text/plain; charset=utf-8"
sync "*.webmanifest" "application/manifest+json; charset=utf-8"
sync "*.js"          "text/javascript; charset=utf-8"
sync "*.json"        "application/json; charset=utf-8"
sync "*.html"        "text/html; charset=utf-8"

# Upload remaining and remove unused
echo "***** If you have nested folder not generated from this build, cancel now *****"
echo "The next command does clean up, and will delete those nexted folders."
aws s3 sync ${SOURCE_DIR} s3://${BUCKET_NAME}${BUCKET_PATH} --sse \
    --delete \
    ${CACHE_CONTROL} ${DRYRUN} \
    --profile ${AWS_PROFILE}

if [ ! -z "${AWS_CLOUDFRONT_DISTRIBUTION_ID}" ]; then
  PATHS_FILES=$(ls -p ${SOURCE_DIR} | grep -v / | awk '$0="'${BUCKET_PATH}'"$0')
  PATHS_DIRS=$(ls -p ${SOURCE_DIR} | grep / | awk '$0="'${BUCKET_PATH}'"$0"*"')
  PATHS="${PATHS_FILES[@]} ${PATHS_DIRS[@]}"

  if [ ! -z "${DRYRUN}" ]; then
    echo "(dryrun) aws cloudfront create-invalidation --profile ${AWS_PROFILE} --distribution-id ${AWS_CLOUDFRONT_DISTRIBUTION_ID} --paths $(echo $PATHS | xargs echo)"
  else
    echo ${PATHS} | xargs aws cloudfront create-invalidation --profile ${AWS_PROFILE} --distribution-id ${AWS_CLOUDFRONT_DISTRIBUTION_ID} --paths
  fi
fi
