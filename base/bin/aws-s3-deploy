#!/usr/bin/env bash
set -e

DEFAULT_CACHE_CONTROL='max-age=31536000, immutable' # or no-cache

# Src: https://stackoverflow.com/questions/192249/how-do-i-parse-command-line-arguments-in-bash
POSITIONAL=()
while [[ $# -gt 0 ]]; do
    case ${1} in
        --dryrun)
        DRYRUN="${1}"
        shift # past argument
        ;;
        --clear-all)
        CLEARALL="/*"
        shift # past argument
        ;;
        --compression)
        COMPRESSION="TRUE"
        shift # past argument
        ;;
        --cache-control)
        CACHE_CONTROL="${2}"
        shift # past argument
        shift # past value
        ;;
        --distribution-id)
        AWS_CLOUDFRONT_DISTRIBUTION_ID="${2}"
        shift # past argument
        shift # past value
        ;;
        --region)
        AWS_REGION="${2}"
        shift # past argument
        shift # past value
        ;;
        --profile)
        AWS_PROFILE="${2}"
        shift # past argument
        shift # past value
        ;;
        *)    # unknown option
        POSITIONAL+=("$1") # save it in an array for later
        shift # past argument
        ;;
    esac
done
set -- "${POSITIONAL[@]}" # restore positional parameters

BUCKET_NAME=$1
SOURCE_DIR=${2:-./}
BUCKET_PATH=${3:-/}

AWS_REGION=${AWS_REGION:-${AWS_DEFAULT_REGION}}
AWS_PROFILE=${AWS_PROFILE:-${AWS_DEFAULT_PROFILE}}

CACHE_CONTROL=${CACHE_CONTROL:-${DEFAULT_CACHE_CONTROL}}

# Ensure required vars a set
if [ -z "${BUCKET_NAME}" ]; then
    echo "Destination bucket name required. 'aws-s3-deploy \${source_dir} \${bucket_name}'";
    exit 1;
fi


# ** Modify below to fit your specific needs **

# Clean up
#aws s3 rm s3://${BUCKET_NAME} --recursive --include "*"

# Docs http://docs.aws.amazon.com/cli/latest/reference/s3/sync.html
# --dryrun (boolean) Displays the operations that would be performed using the specified command without actually running them.
# --quiet (boolean) Does not display the operations performed from the specified command.
# --include (string) Don't exclude files or objects in the command that match the specified pattern. See Use of Exclude and Include Filters for details.
# --exclude (string) Exclude all files or objects from the command that matches the specified pattern.
# --sse (string) Specifies server-side encryption of the object in S3. Valid values are AES256 and aws:kms. If the parameter is specified but no value is provided, AES256 is used.
# --content-type (string) Specify an explicit content type for this operation. This value overrides any guessed mime types.
# --cache-control (string) Specifies caching behavior along the request/reply chain.
# --content-language (string) The language the content is in.
# --expires (string) The date and time at which the object is no longer cacheable.
# --metadata (map) A map of metadata to store with the objects in S3. This will be applied to every object which is part of this request. In a sync, this means that files which haven't changed won't receive the new metadata. When copying between two s3 locations, the metadata-directive argument will default to 'REPLACE' unless otherwise specified. http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-metadata
# --metadata-directive (string) Specifies whether the metadata is copied from the source object or replaced with metadata provided when copying S3 objects. Note that if the object is copied over in parts, the source object's metadata will not be copied over, no matter the value for --metadata-directive, and instead the desired metadata values must be specified as parameters on the command line. Valid values are COPY and REPLACE. If this parameter is not specified, COPY will be used by default. If REPLACE is used, the copied object will only have the metadata values that were specified by the CLI command. Note that if you are using any of the following parameters: --content-type, content-language, --content-encoding, --content-disposition, --cache-control, or --expires, you will need to specify --metadata-directive REPLACE for non-multipart copies if you want the copied objects to have the specified metadata values.
# --delete (boolean) Files that exist in the destination but not in the source are deleted during sync.

echo "s3 sync ${SOURCE_DIR} => s3://${BUCKET_NAME}${BUCKET_PATH}"

clean() {
    find . -type f -name '*.DS_Store' -exec rm {} \;
}

copy() {
    CONTENT_TYPE=${2:-''}
    echo "copy ${1} ${CONTENT_TYPE}"
    aws s3 cp ${SOURCE_DIR} s3://${BUCKET_NAME}${BUCKET_PATH} --recursive --sse \
            --no-guess-mime-type --content-type "${CONTENT_TYPE}" --metadata-directive "REPLACE" \
            --exclude="*" --include "${1}" \
            --cache-control "${CACHE_CONTROL}"${DRYRUN} \
            --profile ${AWS_PROFILE}
}

sync() {
    CONTENT_TYPE=${2:-''}
    echo "sync ${1} ${CONTENT_TYPE}"

    CACHE_CONTROL_OVERRIDE=CACHE_CONTROL
    #if [ "index.html" == "${1}" ] && [ "no-cache" != "${CONTENT_TYPE}" ]; then
    #    CACHE_CONTROL_OVERRIDE='max-age=180'
    #fi

    # Service workers should never be cached
    if [ "sw.js" == "${1}" ]; then
        CACHE_CONTROL_OVERRIDE='no-cache'
    fi

    aws s3 sync ${SOURCE_DIR} s3://${BUCKET_NAME}${BUCKET_PATH} --sse \
            --no-guess-mime-type --content-type "${CONTENT_TYPE}" --metadata-directive "REPLACE" \
            --exclude="*" --include "${1}" \
            --cache-control "${CACHE_CONTROL_OVERRIDE}" ${DRYRUN} \
            --profile ${AWS_PROFILE}

    if [ "${COMPRESSION}" == "TRUE" ]; then
      # brotli
      find ${SOURCE_DIR} -type f -name "${1}" -exec brotli --best --keep --force {} +
      aws s3 sync ${SOURCE_DIR} s3://${BUCKET_NAME}${BUCKET_PATH} --sse \
        --no-guess-mime-type --content-type "${CONTENT_TYPE}" --content-encoding "br" --metadata-directive "REPLACE" \
        --exclude="*" --include "${1}.br" \
        --cache-control "${CACHE_CONTROL}" ${DRYRUN} \
        --profile ${AWS_PROFILE}

      # gzip
      find ${SOURCE_DIR} -type f -name "${1}" -exec gzip --best --keep --force {} +
      aws s3 sync ${SOURCE_DIR} s3://${BUCKET_NAME}${BUCKET_PATH} --sse \
        --no-guess-mime-type --content-type "${CONTENT_TYPE}" --content-encoding "gzip" --metadata-directive "REPLACE" \
        --exclude="*" --include "${1}.gz" \
        --cache-control "${CACHE_CONTROL}" ${DRYRUN} \
        --profile ${AWS_PROFILE}

    fi
}

clean

# /etc/mime.types
# https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Complete_list_of_MIME_types
sync "*.webmanifest" "application/manifest+json; charset=utf-8"
sync "*.json"        "application/json; charset=utf-8"
sync "*.txt"         "text/plain; charset=utf-8"
sync "*.js"          "text/javascript; charset=utf-8"
sync "*.css"         "text/css; charset=utf-8"
sync "*.xml"         "application/xml" # text/xml human readable; application/xml non-human readable
sync "rss.xml"       "application/rss+xml"
sync "atom.xml"      "application/atom+xml"
sync "*.html"        "text/html; charset=utf-8"

# Images
sync "*.ico"   "image/vnd.microsoft.icon"
sync "*.svg"   "image/svg+xml; charset=utf-8"
sync "*.jpg"   "image/jpeg"
sync "*.png"   "image/png"
sync "*.webp"  "image/webp"
sync "*.gif"   "image/gif"

# Video
sync "*.webm"  "video/webm"
sync "*.mp4"   "video/mp4"
sync "*.vtt"   "text/vtt"

# Fonts
sync "*.eot"   "application/vnd.ms-fontobject"
sync "*.ttf"   "font/ttf"
sync "*.woff"  "font/woff"
sync "*.woff2" "font/woff2"

# Downloads
sync "*.pdf"   "application/pdf"
sync "*.csv"   "text/csv"
sync "*.xlsx"  "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
sync "*.zip"   "application/zip"

# Upload remaining and remove unused
echo "***** If you have nested folder not generated from this build, cancel now *****"
echo "The next command does clean up, and will delete those nexted folders."
aws s3 sync ${SOURCE_DIR} s3://${BUCKET_NAME}${BUCKET_PATH} --sse \
        --delete --metadata-directive "REPLACE" \
        --cache-control "${CACHE_CONTROL}" ${DRYRUN} \
        --profile ${AWS_PROFILE}

if [ ! -z "${AWS_CLOUDFRONT_DISTRIBUTION_ID}" ]; then
    PATHS_FILES=$(ls -p ${SOURCE_DIR} | grep -v / | awk '$0="'${BUCKET_PATH}'"$0')
    PATHS_DIRS=$(ls -p ${SOURCE_DIR} | grep / | awk '$0="'${BUCKET_PATH}'"$0"*"')
    PATHS="${PATHS_FILES[@]} ${PATHS_DIRS[@]}"

    if [ ! -z "${DRYRUN}" ]; then
        echo "(dryrun) aws cloudfront create-invalidation --profile ${AWS_PROFILE} --distribution-id ${AWS_CLOUDFRONT_DISTRIBUTION_ID} --paths $(echo $PATHS | xargs echo)"
    elif [ ! -z "${CLEARALL}" ]; then
       aws cloudfront create-invalidation --profile ${AWS_PROFILE} --distribution-id ${AWS_CLOUDFRONT_DISTRIBUTION_ID} --paths "${CLEARALL}"
    else
        echo ${PATHS} | xargs aws cloudfront create-invalidation --profile ${AWS_PROFILE} --distribution-id ${AWS_CLOUDFRONT_DISTRIBUTION_ID} --paths
    fi
fi
